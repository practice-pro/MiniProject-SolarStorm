{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Tr_GkDEBLZ8",
        "outputId": "4c1696fb-a630-4b68-bcf1-f66a1f02c6fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting utils\n",
            "  Downloading utils-1.0.1-py2.py3-none-any.whl (21 kB)\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "oTS4zc28ngo3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "401d27c2-3b7f-477b-b8d0-85c14e500796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOZyQCHhRc3g",
        "outputId": "c7c5c021-6d6c-41e4-cf45-937f2584b893"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9657 sha256=fa003600dbae0ba2c0a0e53bb2d16cb5cb8a9604753510999a14a3b12030f0b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import regex as re\n",
        "import wget\n",
        "import os"
      ],
      "metadata": {
        "id": "9mXL6N7MI3u3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qX2nN5zpzGvp"
      },
      "outputs": [],
      "source": [
        "from re import X\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "from argparse import ArgumentParser, Namespace\n",
        "from pathlib import Path\n",
        "from typing import Dict\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from utils import *\n",
        "import xarray as xr\n",
        "import datetime\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sd = datetime.datetime(2023, 5, 12).strftime(\"%Y%m%d\")\n",
        "ed = datetime.datetime(2023,5,12).strftime(\"%Y%m%d\")\n",
        "\n",
        "d1=datetime.date(2023,5,12)\n",
        "d2=datetime.date(2023,5,12)\n",
        "\n",
        "delta=(d2-d1).days"
      ],
      "metadata": {
        "id": "kvzPrcbTI_CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls=pd.date_range(sd,ed)"
      ],
      "metadata": {
        "id": "hc0l-U_wJLbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dates=[dt.strftime(\"%Y%m%d\") for dt in ls]"
      ],
      "metadata": {
        "id": "QMo--J7DJU1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "years={date[:4] for date in dates}\n",
        "months={date[4:6] for date in dates}"
      ],
      "metadata": {
        "id": "PpdBgVAiJaG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt=\"\"\n",
        "for year in years:\n",
        "    for month in months:\n",
        "        base='https://www.ngdc.noaa.gov/dscovr/data/'+year+'/'+month+'/'\n",
        "        txt+=requests.get(base).text"
      ],
      "metadata": {
        "id": "hV1YRqdBJeHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urllist=[]"
      ],
      "metadata": {
        "id": "Ha_qCQfyJi5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for date in dates:\n",
        "    base='https://www.ngdc.noaa.gov/dscovr/data/'+date[:4]+'/'+date[4:6]+'/'\n",
        "    x=re.search(str(\"oe_m1m_dscovr_s\"+date+\"000000_\"), txt).start()\n",
        "    url=base+txt[x: x+71]\n",
        "    print(date,\": \",url)\n",
        "    urllist.append(url[46:])\n",
        "    wget.download(url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyE0Mi6xJl76",
        "outputId": "2d4e013c-9ecb-4eff-b2f8-38211f3eb030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20230512 :  https://www.ngdc.noaa.gov/dscovr/data/2023/05/oe_m1m_dscovr_s20230512000000_e20230512235959_p20230513021827_pub.nc.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l=len(urllist)"
      ],
      "metadata": {
        "id": "u0nbsMvgJpy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "magdata=[]"
      ],
      "metadata": {
        "id": "1x6x5kGiJ1tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,l):\n",
        "  ds=xr.open_dataset('/content/'+urllist[i])\n",
        "  df=ds.to_dataframe()\n",
        "  actualdf=df.dropna()\n",
        "  df_len=len(actualdf)\n",
        "  for j in range(0,df_len,180):\n",
        "   bx_gse_hour=np.nanmean(np.array(actualdf['bx_gse'][j:j+180]))\n",
        "   by_gse_hour=np.nanmean(np.array(actualdf['by_gse'][j:j+180]))\n",
        "   bz_gse_hour=np.nanmean(np.array(actualdf['bz_gse'][j:j+180]))\n",
        "   b_gse=((bx_gse_hour)**2+(by_gse_hour)**2+(bz_gse_hour)**2)**0.5\n",
        "   if bx_gse_hour+by_gse_hour+bz_gse_hour<0:\n",
        "    magdata.append([-b_gse,-b_gse,-b_gse])\n",
        "   else:\n",
        "    magdata.append([b_gse,b_gse,b_gse])"
      ],
      "metadata": {
        "id": "hrC0-sh9J4V2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "magdata_numpy=np.array(magdata)"
      ],
      "metadata": {
        "id": "FqFNRDUcLSy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "magdata_torch= torch.from_numpy(magdata_numpy)"
      ],
      "metadata": {
        "id": "bxoPhMDBLdjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ml1=magdata_torch.unsqueeze(-1)"
      ],
      "metadata": {
        "id": "p3RIgaLBfLl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kpindexdata=requests.get('https://services.swpc.noaa.gov/text/3-day-geomag-forecast.txt').text"
      ],
      "metadata": {
        "id": "1PLINrro884m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lkp=kpindexdata[470:].strip().split()\n",
        "forecastdata=[]\n",
        "lenkp=len(lkp)\n",
        "for i in range(7,lenkp,4):\n",
        "    forecastdata.append([float(lkp[i]),float(lkp[i+1]),float(lkp[i+2])])"
      ],
      "metadata": {
        "id": "pEHG6F3H93la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fnumpy=np.array(forecastdata)"
      ],
      "metadata": {
        "id": "IKR_QjxSeAgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forecastdata_torch= torch.from_numpy(fnumpy)"
      ],
      "metadata": {
        "id": "PbHNAh1ueSrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ft=forecastdata_torch.unsqueeze(-1)"
      ],
      "metadata": {
        "id": "rUllgP6vhAMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        hidden_dim: int,\n",
        "        output_dim: int,\n",
        "        num_layers: int) -> None:\n",
        "        super(Seq2Seq, self).__init__()\n",
        "\n",
        "        self.rnn = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.dnn = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, seq: torch.Tensor) -> torch.Tensor:\n",
        "        output, hn = self.rnn(seq)\n",
        "        output = self.dnn(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "XlbxHCZfQdaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "CXI1Cxxla7c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DSCOVRKP():\n",
        "\tdef __init__(self):\n",
        "\t\tself.x_prime = torch.tensor([]).float()\n",
        "\t\tself.forecast = torch.tensor([]).float()\n",
        "\t\tself.x_prime = torch.cat([self.x_prime, ml1.float()], dim=0)\n",
        "\t\tself.forecast = torch.cat([self.forecast, ft.float()], dim=0)\n",
        "\t\t\n",
        "\t\tprint(\"total x_prime shape:\", self.x_prime.shape)\n",
        "\t\tprint(\"total forecast shape:\", self.forecast.shape)\n",
        "\n",
        "\n",
        "\tdef __len__(self) -> int:\n",
        "\t\treturn self.forecast.shape[0]\n",
        "\n",
        "\tdef __getitem__(self, index):\n",
        "\t\treturn self.x_prime[index],self.forecast[index]"
      ],
      "metadata": {
        "id": "bmH1fOl8DcMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  d=DSCOVRKP()\n",
        "  data_len = len(d)\n",
        "  train_ratio,test_ratio = 0.7,0.3\n",
        "  train_len= int(data_len*train_ratio)\n",
        "  test_len = data_len - train_len\n",
        "  train_dataset,test_dataset = random_split(d, [train_len,test_len])\n",
        "  print(f\"train,test dataset len: {len(train_dataset)}, {len(test_dataset)}\")\n",
        "  train_loader = DataLoader(train_dataset, 8, shuffle=True)\n",
        "  test_loader = DataLoader(test_dataset, 8, shuffle=False)\n",
        "  model = Seq2Seq(1,3,1,3)\n",
        "  print(model)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.8)\n",
        "  scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, min_lr=5e-5)\n",
        "  loss_train=0\n",
        "  prev_train=100\n",
        "  while abs(loss_train-prev_train)>0.001:\n",
        "    model.train()\n",
        "    prev_train=loss_train\n",
        "    loss_train, acc_train, iterations = 0, 0, 0\n",
        "    for x_prime, forecast in train_loader:\n",
        "      outputs = model(x_prime)\n",
        "      criterion = torch.nn.MSELoss()\n",
        "      newdst=forecast\n",
        "      loss = criterion(outputs, newdst)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      iterations += 1\n",
        "      loss_train += loss.item()\n",
        "    loss_train /= iterations\n",
        "    print(f\"train_loss: {loss_train:.4f}\")\n",
        "    torch.save(model.state_dict(), os.path.join('/content/drive/MyDrive', \"proton_to_forecast1.pt\"))\n",
        "\t\t\n",
        "\n"
      ],
      "metadata": {
        "id": "ENJDBEkH7ydF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89325471-1440-4f2c-c599-1686c9a2fd8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total x_prime shape: torch.Size([8, 3, 1])\n",
            "total forecast shape: torch.Size([8, 3, 1])\n",
            "train,test dataset len: 5, 3\n",
            "Seq2Seq(\n",
            "  (rnn): GRU(1, 3, num_layers=3, batch_first=True)\n",
            "  (dnn): Sequential(\n",
            "    (0): Linear(in_features=3, out_features=1, bias=True)\n",
            "  )\n",
            ")\n",
            "train_loss: 3.3447\n",
            "train_loss: 0.5546\n",
            "train_loss: 0.5381\n",
            "train_loss: 2.1819\n",
            "train_loss: 0.3538\n",
            "train_loss: 1.6235\n",
            "train_loss: 0.8915\n",
            "train_loss: 0.2037\n",
            "train_loss: 0.8103\n",
            "train_loss: 0.7890\n",
            "train_loss: 0.3325\n",
            "train_loss: 0.4520\n",
            "train_loss: 0.7326\n",
            "train_loss: 0.5528\n",
            "train_loss: 0.2877\n",
            "train_loss: 0.3593\n",
            "train_loss: 0.4702\n",
            "train_loss: 0.2961\n",
            "train_loss: 0.1819\n",
            "train_loss: 0.3073\n",
            "train_loss: 0.3781\n",
            "train_loss: 0.2717\n",
            "train_loss: 0.2102\n",
            "train_loss: 0.2909\n",
            "train_loss: 0.3332\n",
            "train_loss: 0.2544\n",
            "train_loss: 0.2031\n",
            "train_loss: 0.2497\n",
            "train_loss: 0.2748\n",
            "train_loss: 0.2196\n",
            "train_loss: 0.1836\n",
            "train_loss: 0.2183\n",
            "train_loss: 0.2323\n",
            "train_loss: 0.1857\n",
            "train_loss: 0.1704\n",
            "train_loss: 0.2005\n",
            "train_loss: 0.1928\n",
            "train_loss: 0.1648\n",
            "train_loss: 0.1823\n",
            "train_loss: 0.1916\n",
            "train_loss: 0.1627\n",
            "train_loss: 0.1610\n",
            "train_loss: 0.1759\n",
            "train_loss: 0.1539\n",
            "train_loss: 0.1470\n",
            "train_loss: 0.1604\n",
            "train_loss: 0.1419\n",
            "train_loss: 0.1455\n",
            "train_loss: 0.1442\n",
            "train_loss: 0.1325\n",
            "train_loss: 0.1448\n",
            "train_loss: 0.1344\n",
            "train_loss: 0.1372\n",
            "train_loss: 0.1355\n",
            "train_loss: 0.1293\n",
            "train_loss: 0.1360\n",
            "train_loss: 0.1293\n",
            "train_loss: 0.1365\n",
            "train_loss: 0.1294\n",
            "train_loss: 0.1352\n",
            "train_loss: 0.1290\n",
            "train_loss: 0.1338\n",
            "train_loss: 0.1287\n",
            "train_loss: 0.1322\n",
            "train_loss: 0.1279\n",
            "train_loss: 0.1302\n",
            "train_loss: 0.1274\n",
            "train_loss: 0.1287\n",
            "train_loss: 0.1267\n",
            "train_loss: 0.1269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "  model = Seq2Seq(1,3,1,3)\n",
        "  model.load_state_dict(torch.load(os.path.join('/content/drive/MyDrive', \"proton_to_forecast1.pt\")))\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    loss_test, acc_test,iterations = 0, 0, 0 \n",
        "    for x_prime, forecast in test_loader:\n",
        "      outputs = model(x_prime)\n",
        "      loss = criterion(outputs,forecast)\n",
        "      iterations += 1\n",
        "      loss_test += loss.item()\n",
        "      for i in range(outputs.shape[0]):\n",
        "       for j in range(outputs.shape[1]):\n",
        "        for k in range(outputs.shape[2]):\n",
        "          if loss_test<1.1 and abs(outputs[i][j][k]-forecast[i][j][k])<=loss_test:\n",
        "            acc_test+=1\n",
        "          elif abs(outputs[i][j][k]-forecast[i][j][k])<1.0:\n",
        "            acc_test+=1\n",
        "    loss_test /= iterations\n",
        "    acc_test /= (outputs.shape[0])*(outputs.shape[1])*(outputs.shape[2])\n",
        "  print(f\"test_acc: {acc_test*100:.4f},test_loss: {loss_test:.4f}\")"
      ],
      "metadata": {
        "id": "SC3EXbPyCnzG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59754129-0904-4654-cb2e-d8d82850e9ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_acc: 88.8889,test_loss: 0.3564\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs"
      ],
      "metadata": {
        "id": "AVGkZQ369iAS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e69a9173-00ed-499a-a39a-e0edab34fefd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[3.0223],\n",
              "         [2.3035],\n",
              "         [1.7580]],\n",
              "\n",
              "        [[2.4949],\n",
              "         [1.8840],\n",
              "         [1.1419]],\n",
              "\n",
              "        [[2.4949],\n",
              "         [1.8840],\n",
              "         [1.1419]]])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    }
  ]
}